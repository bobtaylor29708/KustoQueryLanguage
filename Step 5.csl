// KQL Sample code - Step 5 Time series reporting
 
// This Sample Code is provided for the purpose of illustration only and is not intended to be used in a production environment.  
// THIS SAMPLE CODE AND ANY RELATED INFORMATION ARE PROVIDED "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE IMPLIED WARRANTIES OF MERCHANTABILITY AND/OR FITNESS FOR A PARTICULAR PURPOSE.  
// We grant You a nonexclusive, royalty-free right to use and modify the 
// Sample Code and to reproduce and distribute the object code form of the Sample Code, provided that You agree: (i) to not use Our name, logo, or trademarks to market Your software product in which the Sample Code is embedded; 
// (ii) to include a valid copyright notice on Your software product in which the Sample Code is 
// embedded; and 
// (iii) to indemnify, hold harmless, and defend Us and Our suppliers from and against any claims or lawsuits, including attorneysâ€™ fees, that arise or result from the use or distribution of the Sample Code.
// Please note: None of the conditions outlined in the disclaimer above will supercede the terms and conditions contained within the Premier Customer Services Description.
// 


// range
// Produces a table in steps using the boundaries indicated, incrementing by 
// the value in the step parameter
range integers from 1 to 8 step 1;

range integers from 1 to 8 step 2;

// Works great with dates
range LastWeek from ago(7d) to now() step 1d;

range LastHours from ago(24h) to now() step 1h;


// Here, we'll create a table from a range, one for each day, for the
// start of the day. Then we'll join that to a query getting the 
// number of events bucketed by date
// We'll then add a column that nicely formats the date 
// It then sorts the results and displays them

// I will demo this by using the LogAnalytics demo site
//https://portal.loganalytics.io/demo
range LastWeek from ago(7d) to now() step 1d
| project TimeGenerated = startofday(LastWeek)
| join kind=fullouter ( Perf 
                      | where TimeGenerated > ago(7d)
                      | summarize Count = count() 
                               by bin(TimeGenerated, 1d)
) on TimeGenerated
| extend TimeDisplay = format_datetime(TimeGenerated, "yyyy-MM-dd") 
| sort by TimeGenerated asc
| project TimeDisplay 
        , Count 
| render timechart;


// make-series
// Takes a series of values and converts them to an array of values within a
// column
Perf
| where TimeGenerated > ago(3d)
| where CounterName  == "Available MBytes"
| make-series avg(CounterValue) default=0 
           on TimeGenerated in range(ago(3d), now(), 1h) by Computer;


// We can use make series to generate an array of averages and times, then
// use mvexpand to pivot them back to rows
let startTime=ago(2hour);
let endTime=now();
Perf 
| where TimeGenerated between (startTime .. endTime)
| where CounterName == "% Processor Time" 
    and Computer == "ContosoWeb1.ContosoRetail.com" 
    and ObjectName == "Process" 
    and InstanceName !in ("_Total", "Idle") 
| make-series avg(CounterValue) default=0 
           on TimeGenerated in range(startTime, endTime, 10m) 
           by InstanceName
| mvexpand TimeGenerated to typeof(datetime)
         , avg_CounterValue to typeof(double) 
           limit 100000;


// This query can then be used to render a chart over time
let startTime=ago(2hour);
let endTime=now();
Perf 
| where TimeGenerated between (startTime .. endTime)
| where CounterName == "% Processor Time" 
    and Computer == "ContosoWeb1.ContosoRetail.com" 
    and ObjectName == "Process" 
    and InstanceName !in ("_Total", "Idle") 
| make-series avg(CounterValue) default=0 
           on TimeGenerated in range(startTime, endTime, 10m) 
           by InstanceName
| mvexpand TimeGenerated to typeof(datetime)
         , avg_CounterValue to typeof(double) 
           limit 100000
| render timechart;

// Even better, render has the ability to handle expanding on it's own,
// so the mvexpand step can be eliminated
let startTime=ago(2hour);
let endTime=now();
Perf 
| where TimeGenerated between (startTime .. endTime)
| where CounterName == "% Processor Time" 
    and Computer == "ContosoWeb1.ContosoRetail.com" 
    and ObjectName == "Process" 
    and InstanceName !in ("_Total", "Idle") 
| make-series avg(CounterValue) default=0 
           on TimeGenerated in range(startTime, endTime, 10m) 
           by InstanceName
| render timechart;


// series_stats
// Takes a dynamic series of values and produces a list of all the statistical
// functions for them in one output
let x=dynamic([23,46,23,87,4,8,3,75,2,56,13,75,32,16,29]);
print series_stats(x);

// A variant is series_stats_dynamic, which returns a dynamic column with the
// data in json format
let x=dynamic([23,46,23,87,4,8,3,75,2,56,13,75,32,16,29]);
print series_stats_dynamic(x);


// series_stats_dynamic is used in conjuction with make-series to
// return statistics for a value
Perf
| where TimeGenerated > ago(1d)
| where CounterName == "Available MBytes"
| make-series avg(CounterValue) default=0 
           on TimeGenerated in range(ago(1d), now(), 1h) 
           by Computer
| extend mySeriesStats = series_stats_dynamic(avg_CounterValue); 

// Make it easier to read by projecting the individual values from the
// dyamic value
Perf
| where TimeGenerated > ago(1d)
| where CounterName == "Available MBytes"
| make-series avg(CounterValue) default=0 
           on TimeGenerated in range(ago(1d), now(), 1h) 
           by Computer
| extend mySeriesStats = series_stats_dynamic(avg_CounterValue) 
| project Computer
        , mySeriesStats.min
        , mySeriesStats.min_idx
        , mySeriesStats.max
        , mySeriesStats.max_idx
        , mySeriesStats.avg
        , mySeriesStats.stdev
        , mySeriesStats.variance;
// end of section

//series_outliers
range x from 1 to 100 step 1 
| extend y=iff(x==20 or x==80, 10*rand()+10+(50-x)/2, 10*rand()+10) // generate a sample series with outliers at x=20 and x=80
| summarize x=make_list(x),series=make_list(y)
| extend series_stats(series), outliers=series_outliers(series)
| mv-expand x to typeof(long), series to typeof(double), outliers to typeof(double)
| project x, series , outliers 
| render linechart

//series_outliers with the outliers smoothed out
range x from 1 to 100 step 1 
| extend y=iff(x==20 or x==80, 10*rand()+10+(50-x)/2, 10*rand()+10) // generate a sample series with outliers at x=20 and x=80
| summarize x=make_list(x),series=make_list(y)
| extend series_stats(series), outliers=series_outliers(series)
| mv-expand x to typeof(long), series to typeof(double), outliers to typeof(double)
| project x, series , outliers_removed=iff(outliers > 1.5 or outliers < -1.5, series_stats_series_avg , series ) // replace outliers with the average
| render linechart


//series_decompose for weekly seasonality
let ts=range t from 1 to 24*7*5 step 1 
| extend Timestamp = datetime(2018-03-01 05:00) + 1h * t 
| extend y = 2*rand() + iff((t/24)%7>=5, 10.0, 15.0) - (((t%24)/10)*((t%24)/10)) // generate a series with weekly seasonality
| extend y=iff(t==150 or t==200 or t==780, y-8.0, y) // add some dip outliers
| extend y=iff(t==300 or t==400 or t==600, y+8.0, y) // add some spike outliers
| summarize Timestamp=make_list(Timestamp, 10000),y=make_list(y, 10000);
ts 
| extend series_decompose(y) // Note there are two more parameters which are defaulted to -1, 'avg'. the -1 says automatically detect seasonality, and use avg(x)
| render timechart  ;

// with trend
let ts=range t from 1 to 24*7*5 step 1 
| extend Timestamp = datetime(2018-03-01 05:00) + 1h * t 
| extend y = 2*rand() + iff((t/24)%7>=5, 5.0, 15.0) - (((t%24)/10)*((t%24)/10)) + t/72.0 // generate a series with weekly seasonality and ongoing trend
| extend y=iff(t==150 or t==200 or t==780, y-8.0, y) // add some dip outliers
| extend y=iff(t==300 or t==400 or t==600, y+8.0, y) // add some spike outliers
| summarize Timestamp=make_list(Timestamp, 10000),y=make_list(y, 10000);
ts 
| extend series_decompose(y,-1,"linefit")
| render timechart  ;

// terms -  ad_flag is one of three values +1, -1, or 0 indicating positive anomoly, negative anomoly, or no anomoly
//          baseline the predicted value of the series based on the decomposition
//          ad_score the actual score of the anomoly base on tukey's score

//series_decomposition for weekly seasonality with anomolies
let ts=range t from 1 to 24*7*5 step 1 
| extend Timestamp = datetime(2018-03-01 05:00) + 1h * t 
| extend y = 2*rand() + iff((t/24)%7>=5, 10.0, 15.0) - (((t%24)/10)*((t%24)/10)) // generate a series with weekly seasonality
| extend y=iff(t==150 or t==200 or t==780, y-8.0, y) // add some dip outliers
| extend y=iff(t==300 or t==400 or t==600, y+8.0, y) // add some spike outliers
| summarize Timestamp=make_list(Timestamp, 10000),y=make_list(y, 10000);
ts 
| extend series_decompose_anomalies(y)
| render timechart ;

//series_decomposition for weekly seasonality with anomolies
let ts=range t from 1 to 24*7*5 step 1 
| extend Timestamp = datetime(2018-03-01 05:00) + 1h * t 
| extend y = 2*rand() + iff((t/24)%7>=5, 5.0, 15.0) - (((t%24)/10)*((t%24)/10)) + t/72.0 // generate a series with weekly seasonality and ongoing trend
| extend y=iff(t==150 or t==200 or t==780, y-8.0, y) // add some dip outliers
| extend y=iff(t==300 or t==400 or t==600, y+8.0, y) // add some spike outliers
| summarize Timestamp=make_list(Timestamp, 10000),y=make_list(y, 10000);
ts 
| extend series_decompose_anomalies(y)
| extend series_decompose_anomalies_y_ad_flag = 
series_multiply(10, series_decompose_anomalies_y_ad_flag) // multiply by 10 for visualization purposes
| render timechart  ;
;

// now with line fit parameter
// you can also adjust the threshold value say from 1.5 to 2.5
let ts=range t from 1 to 24*7*5 step 1 
| extend Timestamp = datetime(2018-03-01 05:00) + 1h * t 
| extend y = 2*rand() + iff((t/24)%7>=5, 5.0, 15.0) - (((t%24)/10)*((t%24)/10)) + t/72.0 // generate a series with weekly seasonality and ongoing trend
| extend y=iff(t==150 or t==200 or t==780, y-8.0, y) // add some dip outliers
| extend y=iff(t==300 or t==400 or t==600, y+8.0, y) // add some spike outliers
| summarize Timestamp=make_list(Timestamp, 10000),y=make_list(y, 10000);
ts 
| extend series_decompose_anomalies(y, 2.5, -1, 'linefit') // (series, threshold, seasonality (-1 auto), trend) - there are more check docs
| extend series_decompose_anomalies_y_ad_flag = 
series_multiply(10, series_decompose_anomalies_y_ad_flag) // multiply by 10 for visualization purposes
| render timechart;

// now lets try series_decompose_forecast to fill in the last week
let ts=range t from 1 to 24*7*4 step 1 // generate 4 weeks of hourly data
| extend Timestamp = datetime(2018-03-01 05:00) + 1h * t 
| extend y = 2*rand() + iff((t/24)%7>=5, 5.0, 15.0) - (((t%24)/10)*((t%24)/10)) + t/72.0 // generate a series with weekly seasonality and ongoing trend
| extend y=iff(t==150 or t==200 or t==780, y-8.0, y) // add some dip outliers
| extend y=iff(t==300 or t==400 or t==600, y+8.0, y) // add some spike outliers
| make-series y=max(y) on Timestamp in range(datetime(2018-03-01 05:00), datetime(2018-03-01 05:00)+24*7*5h, 1h); // create a time series of 5 weeks (last week is empty)
ts 
| extend y_forcasted = series_decompose_forecast(y, 24*7)  // forecast a week forward
| render timechart;


print y=dynamic([80,139,87,110,68,54,50,51,53,133,86,141,97,156,94,149,95,140,77,61,50,54,47,133,72,152,94,148,105,162,101,160,87,63,53,55,54,151,103,189,108,183,113,175,113,178,90,71,62,62,65,165,109,181,115,182,121,178,114,170])
| project x=range(1, array_length(y), 1), y  
| render linechart

// series_periods_detect
// Important
// The algorithm can detect periods containing at least 4 points and at most half of the series length.
// You should set the min_period a little below and max_period a little above the periods you expect to find in the time series. 
// For example, if you have an hourly-aggregated signal, and you look for both daily > and weekly periods (that would be 24 & 168 respectively) 
// you can set min_period=0.8*24, max_period=1.2*168, leaving 20% margins around these periods.
// The input time series must be regular, i.e. aggregated in constant bins (which is always the case if it has been created using make-series). 
// Otherwise, the output is meaningless.
// These data points equate to readings every 12 hours. So in this case series_periods_detect will find a period of 14 which is (12h x 7 days)

print y=dynamic([80,139,87,110,68,54,50,51,53,133,86,141,97,156,94,149,95,140,77,61,50,54,47,133,72,152,94,148,105,162,101,160,87,63,53,55,54,151,103,189,108,183,113,175,113,178,90,71,62,62,65,165,109,181,115,182,121,178,114,170])
| project x=range(1, array_length(y), 1), y  
| project series_periods_detect(y, 0.0, 50.0, 1) //( array, min period, max period, number of periods

//
// end of sesssion

// series_fir
// FIR Is Finite Impulse Repsonse time. It is typically used in digital 
// signal processing, such as that used in radio (especially amateur radio). 
// The finite indicates the array of values, if continued, would eventually
// dwindle down to a zero value.
// FIR analysis is a specialized discipline, however we can use FIR to assist
// with other more common processing needs. 

// Show a moving average of last 5 values
range t from bin(now(), 1h)-23h to bin(now(), 1h) step 1h
| summarize t=make_list(t)
| project id='TS', val=dynamic([0,0,0,0,0,0,0,0,0,10,20,40,100,40,20,10,0,0,0,0,0,0,0,0]), t
| extend 5h_MovingAvg=series_fir(val, dynamic([1,1,1,1,1])),
         5h_MovingAvg_centered=series_fir(val, dynamic([1,1,1,1,1]), true, true)
| render timechart;


range t from bin(now(), 1h)-23h to bin(now(), 1h) step 1h
| summarize t=makelist(t)
| project val=dynamic([10,20,30,40,5,6,7,8,0,10,20,40,100,40,20,10,20,9,7,20,80,10,5,1]), t
| extend 5h_MovingAvg=series_fir(val, dynamic([1,1,1,1,1])),
         5h_MovingAvg_centered=series_fir(val, dynamic([1,1,1,1,1]), true, true) // note could have been dynamic([0.20,0.20,0.20,0.20,0.20]), false, false
| mvexpand val, t, 5h_MovingAvg, 5h_MovingAvg_centered     


// Show difference between current value and previous value
range t from bin(now(), 1h)-11h to bin(now(), 1h) step 1h
| summarize t=makelist(t)
| project t,value=dynamic([1,2,4,6,2,2,2,2,3,3,3,3])
| extend diff=series_fir(value, dynamic([1,-1]), false, false) // notice the 1, -1 which says compare against previous value
| mvexpand t, value, diff;

// Now to do something more useful. Get a list of High CPU Alerts for a 
// computer, then for each alert calculate the time since the previous 
// alert. Zero out the first row since that data would be misleading.
let filterOutBigValues = (val:real)
{
  iif( val > 10000000, 0.0, val)
};
let convertToMinutes = (val:real)
{
  // 1 sec = 10000000 ns
  round( (val / 10000000) / 60.0, 2)   
};
let convertMinToHours = (val:real)
{
  round(val / 60.0, 2)
};
Alert
| where TimeGenerated >= ago(90d)
| where AlertName == "Computers with high processor usage [Log alert on Log Analytics data]"
| where Computer == "Data06"
| project Computer 
        , TimeGenerated 
| order by TimeGenerated asc
| summarize tg=makelist(tolong(TimeGenerated)) by Computer
| extend diff=series_fir(tg,  dynamic([1, -1]), false, false)
| mvexpand tg, diff
| extend TimeGenerated = todatetime(tg) 
| extend diffInMinutes = convertToMinutes(diff)
| extend diffInHours = convertMinToHours(diffInMinutes)
| extend DifferenceInMinutes = filterOutBigValues(diffInMinutes)
       , DifferenceInHours =  filterOutBigValues(diffInHours)  
| project-away tg, diffInMinutes, diffInHours; 


//------------------------------------------------------------------------------
// series_iir
//------------------------------------------------------------------------------

// Similar to FIR, IIR is Infinite Impulse Response. Like FIR, it is most
// commonly used in signal processing. The major difference between FIR and IIR
// is that the range of values is assumed to go on infinately, rather than 
// than FIRs assumption it will be declining to zero 

// Like FIR, IIR can be used for similar functions. 

// Use IIR to calculate cumulative values
range t from bin(now(), 1h)-23h to bin(now(), 1h) step 1h
| summarize t=makelist(t)
| project val=dynamic([10,20,30,40,5,6,7,8,0,10,20,40,100,40,20,10,20,9,7,20,80,10,5,1]), t
| extend CumulativeTotal=series_iir(val, dynamic([1]), dynamic([1,-1])    )
| mvexpand val, t, CumulativeTotal         


// Here, we will determine the number of alerts for a computer on a given 
// day, then accumulate them using series_iir
Alert
| where TimeGenerated >= ago(90d)
| where AlertName == "Computers with high processor usage [Log alert on Log Analytics data]"
| where Computer == "Data06"
| summarize AlertCount=count() by bin(TimeGenerated, 1d) 
| order by TimeGenerated asc
| summarize ac=makelist(AlertCount), tg=makelist(TimeGenerated)
| extend CumulativeAlertCount = series_iir(ac,  dynamic([1]),  dynamic([1, -1]) )
| mvexpand tg, ac, CumulativeAlertCount
| extend AlertDate = format_datetime(todatetime(tg), 'yyyy-MM-dd') 
| project AlertDate, DailyAlertCount = ac, CumulativeAlertCount ;


// With a slight variation to our query, we can now render this as a chart,
// making it much easier to spot any spikes in alert counts
Alert
| where TimeGenerated >= ago(90d)
| where AlertName == "Computers with high processor usage [Log alert on Log Analytics data]"
| where Computer == "Data06"
| summarize AlertCount=count() by bin(TimeGenerated, 1d) 
| order by TimeGenerated asc
| summarize ac=makelist(AlertCount), tg=makelist(TimeGenerated)
| extend CumulativeAlertCount = series_iir(ac,  dynamic([1]),  dynamic([1, -1]) )
| mvexpand tg, ac, CumulativeAlertCount
| extend AlertDate = todatetime(tg)
       , CumulativeAlertCount = toint(CumulativeAlertCount) 
| project AlertDate, CumulativeAlertCount
| render timechart 

//------------------------------------------------------------------------------
// series_fit_line
//------------------------------------------------------------------------------

// series_fit_line performs a linear regression on a series. It returns
// multiple values:
// RSquare: Standard measure of the fit of quality, in a range of 0 to 1. 
//          the closer to 1, the better the fit.
// Slope:   Slope of the approximated line
// Variance: The variance of the input data
// RVariance: Residual variance (the variance between the input data)
// Interception: Interception of the approcimated line
// Line_Fit: Numericial array holding the values of the best fit line.
//           Typically used for charting.
range x from 1 to 1 step 1
| project x = range(bin(now(), 1h)-11h, bin(now(), 1h), 1h)
        , y = dynamic([2,5,6,8,11,15,17,18,25,26,30,30])
| extend (RSquare,Slope,Variance,RVariance,Interception,LineFit) = series_fit_line(y);

// Render this as charted data
// y represents the actual value
// LineFit represents the best fit "predicted" value
range x from 1 to 1 step 1
| project x=range(bin(now(), 1h)-11h, bin(now(), 1h), 1h)
        , y=dynamic([2,5,6,8,11,15,17,18,25,26,30,30])
| extend (RSquare,Slope,Variance,RVariance,Interception,LineFit) = series_fit_line(y)
| render timechart;


// Let's render a chart based on the total memory (in kb) used per hour
Perf
| where TimeGenerated > ago(1d)
| where CounterName == "Used Memory kBytes" 
| where CounterValue > 0
| make-series TotalMemoryUsed=sum(CounterValue) default=0 
           on TimeGenerated 
           in range(ago(1d), now(), 1h) 
           by Computer
| extend (RSquare,Slope,Variance,RVariance,Interception,LineFit)=series_fit_line(TotalMemoryUsed)
| render timechart;


//------------------------------------------------------------------------------
// series_fit_2lines
//------------------------------------------------------------------------------

// series_fit_2lines takes the input data and splits the collection (it uses
// the terms 'right side' and 'left side' to differentiate). It then
// performs an anaylsis on each part of the data. The best split is the one
// with the maximized RSquare. It will return the best values, but you
// can also return the right and left side values if you want

        
print id=' ', x=range(bin(now(), 1h)-11h, bin(now(), 1h), 1h), y=dynamic([1,2.2, 2.5, 4.7, 5.0, 12, 10.3, 10.3, 9, 8.3, 6.2])
| extend (Slope,Interception,RSquare,Variance,RVariance,LineFit)=series_fit_line(y), (RSquare2, SplitIdx, Variance2,RVariance2,LineFit2)=series_fit_2lines(y)
| project id, x, y, LineFit, LineFit2
| render timechart

         